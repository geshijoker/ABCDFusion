{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d05ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b06217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import yaml\n",
    "import configparser\n",
    "import copy\n",
    "import math \n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10916d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "# mpl.use('Agg')\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "matplotlib.rcParams['lines.linewidth'] = 1\n",
    "matplotlib.rcParams['lines.markersize'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03669a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e745788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abcdfusion\n",
    "from abcdfusion import get_abcd, metrics, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f609e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c2814",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd9d8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/geshi/ABCDFusion\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = '../'\n",
    "try: \n",
    "    os.chdir(ROOT_PATH)\n",
    "    sys.path.insert(0, ROOT_PATH)\n",
    "    print(\"Current working directory: {}\".format(os.getcwd()))\n",
    "except Exception:\n",
    "    print(\"Directory: {} is not valid\".format(ROOT_PATH))\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc28e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and parse config \n",
    "config_file = './configs.yaml'\n",
    "with open(config_file, 'r') as infile:\n",
    "    try:\n",
    "        configs = yaml.safe_load(infile)\n",
    "    except yaml.YAMLError as exc:\n",
    "        sys.exit(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3f38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary = configs['Auxiliary']\n",
    "DATA_PATH = auxiliary['DATA_PATH']\n",
    "\n",
    "OTHER_DATA = auxiliary['OTHER_DATA'] \n",
    "DTI_DATA = auxiliary['DTI_DATA'] \n",
    "RS_DATA = auxiliary['RS_DATA']\n",
    "OUTCOME = auxiliary['OUTCOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80e33789",
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_file = os.path.join(DATA_PATH, DTI_DATA)\n",
    "rs_file = os.path.join(DATA_PATH, RS_DATA)\n",
    "other_file = os.path.join(DATA_PATH, OTHER_DATA)\n",
    "label_file = os.path.join(DATA_PATH, OUTCOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263d03f",
   "metadata": {},
   "source": [
    "# 2. Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8de2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size=0.2\n",
    "batch_size=128\n",
    "num_splits=5\n",
    "num_workers=0\n",
    "num_epochs=10\n",
    "step_size=10\n",
    "lr=0.001\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1387eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset, batch_size, num_workers=0, valid_size=0.2, shuffle=True):\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_data = len(dataset)\n",
    "    if shuffle:\n",
    "        indices = list(range(num_data))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_data))\n",
    "    valid_idx, train_idx = indices[:split], indices[split:]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # load training data in batches\n",
    "    train_loader = DataLoader(dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              sampler=train_sampler,\n",
    "                              num_workers=num_workers)\n",
    "    \n",
    "    # load validation data in batches\n",
    "    valid_loader = DataLoader(dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              sampler=valid_sampler,\n",
    "                              num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "597381ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "abcd_dataset = get_abcd(dti_file, rs_file, other_file, label_file)\n",
    "train_loader, valid_loader = create_datasets(abcd_dataset, batch_size, num_workers, valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fd39faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dti:  torch.Size([128, 31]) rs fmri:  torch.Size([128, 270]) other data:  torch.Size([128, 7]) label:  torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "dti, rs, other, y = next(iter(valid_loader))\n",
    "print('dti: ', dti.shape, 'rs fmri: ', rs.shape, 'other data: ', other.shape, 'label: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754d213",
   "metadata": {},
   "source": [
    "# 3. Define Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, num_classes, criterion, optimizer, scheduler, device):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0\n",
    "    epoch_ce = 0.0\n",
    "    epoch_iou = 0.0\n",
    "    epoch_dice = 0.0\n",
    "    count = 0\n",
    "\n",
    "    piter = tqdm(dataloader, desc='Batch', unit='batch', position=1, leave=False)\n",
    "    for inputs, seg_masks in piter:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "            # transfer label to device\n",
    "        targets = target.to(device)\n",
    "        seg_masks = seg_masks.to(device)\n",
    "        _, targets = torch.max(seg_masks, 1)\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        nxt_count = count+batch_size\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, seg_masks)\n",
    "        \n",
    "                loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        epoch_loss = loss.item() * batch_size/nxt_count + epoch_loss * count/nxt_count\n",
    "        epoch_acc = ((preds == targets).sum()/np.prod(preds.size())).item() * batch_size/nxt_count + epoch_acc * count/nxt_count\n",
    "\n",
    "        count = nxt_count\n",
    "        piter.set_postfix(accuracy=100. * epoch_acc)\n",
    "\n",
    "    epoch_acc *= 100.\n",
    "    scheduler.step()\n",
    "    train_stats = {\n",
    "        'train_loss': epoch_loss,\n",
    "        'train_acc': epoch_acc,\n",
    "    }\n",
    "    \n",
    "    return model, epoch_loss, epoch_acc, train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f87a9",
   "metadata": {},
   "source": [
    "# 4. DTI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56ea9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 31)\n",
    "dti_model = models.BinaryMLP(31, [32, 64, 32], p=0.2)\n",
    "out = dti_model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9371b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(dti_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d463eacc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-08032fcb20e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdti_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3093\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3095\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, _, _, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = dti_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73a24a",
   "metadata": {},
   "source": [
    "# 5. rs fMRI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ed7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
